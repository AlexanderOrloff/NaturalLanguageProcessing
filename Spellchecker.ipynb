{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import wikipedia\n",
    "from string import punctuation\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "from collections import Counter\n",
    "import textdistance\n",
    "punct = set(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [sent.split() for sent in open('corpus_ng.txt', encoding='utf8').read().splitlines()]\n",
    "WORDS = Counter()\n",
    "for sent in corpus:\n",
    "    WORDS.update(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(WORDS.keys())\n",
    "id2word = {i:word for i, word in enumerate(vocab)}\n",
    "\n",
    "vec = TfidfVectorizer(analyzer='char', ngram_range=(1,1))\n",
    "X = vec.fit_transform(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_match_vec(text, X, vec, TOPN=5):\n",
    "    v = vec.transform([text])\n",
    "    similarities = cosine_distances(v, X)\n",
    "    topn = similarities.argsort()[0][:TOPN]\n",
    "    \n",
    "    return [id2word[top] for top in topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_match_with_metric(text, X, vec, metric=textdistance.levenshtein):\n",
    "    similarities = Counter()\n",
    "    lookup = get_closest_match_vec(text, X, vec, TOPN=3)\n",
    "    for word in lookup:\n",
    "        similarities[word] = metric.normalized_similarity(text, word) \n",
    "    \n",
    "    return similarities.most_common(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('апофеоз', 0.7142857142857143)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_closest_match_with_metric('опофиоз', X, vec, textdistance.hamming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы создали функцию, которая сначала находит пять ближайших соответствий по символьному векторному представлению слов, а затем, для этих пяти считает расстояние Левенштейна и выделяет наилучший вариант. Теперь проверим качество её работы на материалах Диалога."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('corpus_ng.txt', encoding='utf8').read().splitlines()\n",
    "vocab = set()\n",
    "for line in corpus:\n",
    "    for word in line.split():       \n",
    "        vocab.add(word.lower())\n",
    "# Лишний???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = open('sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
    "true = open('correct_sents.txt', encoding='utf8').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_words(sent_1, sent_2):\n",
    "    punct = set(punctuation)\n",
    "    tokens_1 = sent_1.lower().split()\n",
    "    tokens_2 = sent_2.lower().split()\n",
    "    \n",
    "    tokens_1 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_1 if (set(token)-punct)]\n",
    "    tokens_2 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_2 if (set(token)-punct)]\n",
    "    \n",
    "    return list(zip(tokens_1, tokens_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakes = []\n",
    "total = 0\n",
    "\n",
    "for i in range(len(true)):\n",
    "    \n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    \n",
    "    for pair in word_pairs:\n",
    "        if pair[0] != pair[1]:\n",
    "            mistakes.append(pair)\n",
    "            total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "failed = []\n",
    "for pair in mistakes:\n",
    "    correction = get_closest_match_with_metric(pair[1], X, vec, textdistance.hamming)[0]\n",
    "    if correction == pair[0]:\n",
    "        correct += 1\n",
    "    else:\n",
    "        failed.append(pair[1])\n",
    "print(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2996168582375479\n"
     ]
    }
   ],
   "source": [
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "т. е. наша функция справилась только с 30 % ошибок из корпуса. На первый взгляд, это значение может показаться не слишком большим. Однако если вспомнить, что мы работаем на материалах газетного корпуса, а лексика СМИ всё же достаточно ограничена, это не так уж и плохо.Посмотрим на ошибки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['симпатичнейшое', 'пояним', 'полчатся', 'нащщот', 'основая', 'вобщем', 'как', 'вы', 'знаете', 'из', 'моего', 'не', 'давнего', 'ящека', 'хороше', 'патаму', 'шта', 'поффтыкав', 'соре', 'чтото', 'мошный', 'хороше', 'седня', 'вешать', 'эт', 'канешна', 'начальнег', 'павзрослому', 'подсаживаеться', 'какието', 'малолетнии', 'монголойдом', 'баръер', 'коментариев', 'отвественный', 'расчитаны', 'обълись', 'распрашивая', 'зубодробительня', 'каааак', 'расщифровать', 'самойто', 'тул', 'ваще', 'тока', 'навернео', 'ооочень', 'както', 'мущщину', 'никада']\n"
     ]
    }
   ],
   "source": [
    "print(failed[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часть из этих ошибок (гуливера, афальтоукладчик, начальнег) неисправимы на нашем корпусе (да и на любом маленьком корпусе), часть (любви, женщины) -, по-видимому, ошибки в выборе падежа и тоже нами не покроются, но некоторые вещи можно исправить увеличением N-грамм (ооооочень)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Программа ниже отличается заменой  TfidfVectorizer(analyzer='char', ngram_range=(1,1)) на vec = CountVectorizer(analyzer='char', ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(WORDS.keys())\n",
    "id2word = {i:word for i, word in enumerate(vocab)}\n",
    "\n",
    "vec = CountVectorizer(analyzer='char', ngram_range=(1,3))\n",
    "X = vec.fit_transform(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "failed = []\n",
    "for pair in mistakes:\n",
    "    correction = get_closest_match_with_metric(pair[1], X, vec, textdistance.hamming)[0]\n",
    "    if correction == pair[0]:\n",
    "        correct += 1\n",
    "    else:\n",
    "        failed.append(pair[1])\n",
    "print(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39846743295019155\n"
     ]
    }
   ],
   "source": [
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ура! мы добились увелечения точности на 10 %. Теперь добавим новый корпус. Так у нас есть небольшой корпус из википедии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.set_lang('ru')\n",
    "wiki_content = []\n",
    "pages = wikipedia.random(500)\n",
    "    \n",
    "for page_name in pages:\n",
    "    try:\n",
    "        page = wikipedia.page(page_name)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print('Skipping page {}'.format(page_name), e)\n",
    "        continue\n",
    "    wiki_content.append('{}\\n{}'.format(page.title, page.content.replace('==', '')))\n",
    "for sent in wiki_content.split():\n",
    "    WORDS.update(sent)\n",
    "    \n",
    "for sent in wiki_content.split():\n",
    "    WORDS.update(sent)\n",
    "\n",
    "# слишком сложно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я хотел скачать 500 статей с Вики, но, видимо, моему компу это слишком сложно, так что воспользуемся имеющейся сотней и докинем её в наш корпус"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_texts = json.loads(open('wiki_texts.json', 'r', encoding='utf8').read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = str.maketrans({ch: None for ch in punctuation})\n",
    "for sent in wiki_texts['ru']:\n",
    "    for word in  sent.split():\n",
    "        word = word.translate(table)\n",
    "        WORDS.update(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(WORDS.keys())\n",
    "id2word = {i:word for i, word in enumerate(vocab)}\n",
    "\n",
    "vec = CountVectorizer(analyzer='char', ngram_range=(1,3))\n",
    "X = vec.fit_transform(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "failed = []\n",
    "for pair in mistakes:\n",
    "    correction = get_closest_match_with_metric(pair[1], X, vec, textdistance.hamming)[0]\n",
    "    if correction == pair[0]:\n",
    "        correct += 1\n",
    "    else:\n",
    "        failed.append(pair[1])\n",
    "print(correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как ни странно корпус из википедии на этих данных нам нисколько не помог. Результат почти тот же. Может, просто он недостаточно большой?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь возьмём материалы \"Диалога\" и, исходя из предположения о том, что люди ошибаются не в каждом слове, но спеллчекер может пометить верные слова как неверные, прогоним его через наш алгоритм (добавив еще один уровень, определяющий верность-неверность слова). Будем считать, что материалы \"Диалога\" - симмуляция того, как часто люди ошибаются. Какой процент верного текста мы получим?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent = 0\n",
    "all = 0\n",
    "fail = []\n",
    "def final(word, vocab, X, vec):\n",
    "         if word in vocab:\n",
    "            return word\n",
    "         else:\n",
    "            return get_closest_match_with_metric(word, X, vec, textdistance.hamming)[0]\n",
    "\n",
    "for layer1, layer2 in zip(true, bad):\n",
    "    for item in align_words(layer1, layer2):\n",
    "        if item[0] == final(item[1], vocab, X, vec):\n",
    "            percent += 1\n",
    "        else:\n",
    "            fail.append((item[0], item[1], final(item[1], vocab, X, vec)))\n",
    "        all +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8282\n"
     ]
    }
   ],
   "source": [
    "print(percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8272073511785857\n"
     ]
    }
   ],
   "source": [
    "print(percent/all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть при нашей помощи только 83 % текста в итоге будут верными. Посмотрим на то, что не удалось исправить. Во многом это вещи связанные с, по-видимому, отсутсвием более редких форм типа \"симпатичнейшее\" или \"подсаживается\" в словаре и неправильным выбором  части речи (\"хороше\" стало \"хорошее\", а не \"хорошо\" (с точки зрения парсера оба варианта почти однаково логичны)). Эти проблемы могли бы быть решены подключением синтаксических парсеров, а также, возможно, частично майстемом, но этим мы сейчас заниматься не будем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('симпатичнейшее', 'симпатичнейшое', 'симпатичный'), ('шпионское', 'шпионское', 'шпионской'), ('гламурный', 'гламурный', 'гламур'), ('бонда', 'бонда', 'фонда'), ('superheadz', 'superheadz', 'super'), ('clap', 'clap', 'ap'), ('camera', 'camera', 'america'), ('поясним', 'пояним', 'помним'), ('получатся', 'полчатся', 'полчаса'), ('язычки', 'язычки', 'язычка'), ('милые', 'милые', 'милы'), ('насчет', 'нащщот', 'нащокина'), ('чавеса', 'чавеса', 'чавес'), ('основная', 'основая', 'основа'), ('попавшим', 'попавшим', 'попавших'), ('аварийно-спасательных', 'аварийно-спасательных', 'аварийно-восстановительных'), ('в', 'вобщем', 'всеобщем'), ('общем', 'как', 'как'), ('как', 'вы', 'вы'), ('вы', 'знаете', 'знаете'), ('знаете', 'из', 'из'), ('из', 'моего', 'моего'), ('моего', 'не', 'не'), ('недавнего', 'давнего', 'давнего'), ('пропажу', 'пропажу', 'пропал'), ('ящика', 'ящека', 'щеках'), ('почте.ру', 'почте.ру', 'почте'), ('хорошо', 'хороше', 'хорошее'), ('рите', 'рите', 'жрите'), ('потому', 'патаму', 'датам'), ('что', 'шта', 'штат'), ('переждать', 'переждать', 'упреждать'), ('дубраве', 'дубраве', 'дубрава'), ('люминала', 'люминала', 'финала'), ('повтыкав', 'поффтыкав', 'тыка'), ('билетным', 'билетным', 'билет'), ('кассам', 'кассам', 'касса'), ('ссоре', 'соре', 'сор'), ('что-то', 'чтото', 'что'), ('мощный', 'мошный', 'мош'), ('нерабочем', 'нерабочем', 'рабочем'), ('кредиток', 'кредиток', 'кредитов'), ('хорошо', 'хороше', 'хорошее'), ('капулетти', 'капулетти', 'капулети'), ('прислужницу', 'прислужницу', 'прислуги'), ('кормилицу', 'кормилицу', 'кормилица'), ('сегодня', 'седня', 'средняя'), ('притащиться', 'притащиться', 'тащиться'), ('программист', 'программист', 'программисты'), ('навешать', 'вешать', 'вешать')]\n"
     ]
    }
   ],
   "source": [
    "print(fail[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
